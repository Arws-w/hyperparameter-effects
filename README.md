本次探究超参数影响使用的深度神经网络模型已经上传完毕

一、探究学习率（learning rate）对模型的影响

学习率（learning rate）是训练模型中最为重要的超参数之一，指在优化算法中更新网络权重的幅度大小。学习率可以是恒定的、逐渐降低的，基于动量的或者是自适应的，不同的优化算法决定不同的学习率。为了能够使得梯度下降法有较好的性能，我们需要把学习率的值设定在合适的范围内。可以非常直观地看出学习率（learning rate）越大，权重更新的跨度越大，模型参数调整变化越快。
学习率过小，会极大降低收敛速度，增加训练时间；学习率过大，可能导致参数在最优解两侧来回振荡。

二、探究迭代次数对模型的影响


迭代次数是指整个训练集输入到神经网络进行训练的次数。
当测试错误率和训练错误率相差较小时，可认为当前迭代次数合适；当测试错误率先变小后变大时则说明迭代次数过大，需要减小迭代次数，否则容易出现过拟合。
其中迭代次数越多，训练所用时间越长。

在使用Batch_size方法训练模型时，batch_size对模型的影响在于模型每次更新时，计算所得的梯度是计算整个batch的平均梯度。
batch_size参数决定了完整训练数据集1个epoch需要多少个batch。
若batch_size增加，训练时间会减少，稳定性会提高。同样的epoch数目，在性能允许情况下，大的batchsize需要的batch数目减少了，所以可以减少训练时间。另一方面，大的batch_size梯度的计算更加稳定，因为模型训练曲线会更加平滑。在微调的时候，大的batch_size可能会取得更好的结果。
但是过大的batch_size会导致泛化能力下降。在一定范围内，增加batchsize有助于收敛的稳定性，但是随着batchsize的增加，模型的性能会下降。



在本次探究中，最后发现当learning rate = 0.0001， epoch = 100时，模型在测试集上表现较好。
